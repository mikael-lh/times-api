# Archive ingestion: Archive API → transform → upload to GCS
# Manual trigger only. Resumes from GCS on re-run: skips months already in the bucket.
# Full 100 years (1920–2019). Job timeout 6h; re-run to resume from GCS. Set vars: GCS_BUCKET, GCS_PREFIX.
name: Archive ingest (Archive API → GCS)

on:
  workflow_dispatch:

env:
  GCS_PREFIX: ${{ vars.GCS_PREFIX }}
  GCS_BUCKET: ${{ vars.GCS_BUCKET }}

jobs:
  ingest-and-upload:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max for GitHub-hosted
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Install dependencies
        run: uv sync

      - name: Create .env with API key
        run: echo "NYTIMES_API_KEY=${{ secrets.NYTIMES_API_KEY }}" > .env

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY_INGEST }}'

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v3

      - name: Ingest archive (raw JSON per month, skips months already in GCS)
        run: uv run python -m archive.ingest
        env:
          GCS_BUCKET: ${{ vars.GCS_BUCKET }}
          GCS_PREFIX: ${{ env.GCS_PREFIX }}
          ARCHIVE_MAX_REQUESTS: "500"

      - name: Transform to slim NDJSON
        run: uv run python -m archive.transform

      - name: Upload to GCS
        run: |
          BUCKET="${{ vars.GCS_BUCKET }}"
          PREFIX="${{ vars.GCS_PREFIX }}"
          if [ -z "$BUCKET" ]; then echo "GCS_BUCKET variable is not set"; exit 1; fi
          if [ -z "$PREFIX" ]; then echo "GCS_PREFIX variable is not set"; exit 1; fi
          gsutil -m cp -r archive_raw "gs://${BUCKET}/${PREFIX}/"
          gsutil -m cp -r archive_slim "gs://${BUCKET}/${PREFIX}/"
